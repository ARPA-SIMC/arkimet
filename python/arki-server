#!/usr/bin/python3

import re
import os
import io
import configparser
import logging
import tempfile
import argparse
import datetime
import sys
import arkimet as arki
from werkzeug.exceptions import HTTPException, NotFound
from socketserver import ForkingMixIn
from http.server import HTTPServer, BaseHTTPRequestHandler


class Server(ForkingMixIn, HTTPServer):
    def __init__(self, *args, **kw):
        super().__init__(*args, **kw)
        from werkzeug.routing import Map, Rule
        self.url = "http://{s.server_name}:{s.server_port}".format(s=self)
        self.url_map = Map([
            Rule('/', endpoint='arki_index'),
            Rule('/config', endpoint='arki_config'),
            Rule('/qexpand', endpoint='arki_qexpand'),
            Rule('/dataset/<name>/query', endpoint='arki_dataset_query'),
            Rule('/dataset/<name>/config', endpoint='arki_dataset_config'),
        ])

    def process_request(self, *args, **kw):
        super().process_request(*args, **kw)

    def read_config(self, pathname):
        logging.info("Reading configuration from %s", pathname)
        self.cfg = configparser.ConfigParser()
        self.cfg.read(pathname)

        # Amend configuration turning local datasets into remote dataset
        self.remote_cfg = configparser.ConfigParser()
        for sec in self.cfg.sections():
            self.remote_cfg.add_section(sec)
            for k, v in self.cfg.items(sec):
                self.remote_cfg.set(sec, k, v)
            self.remote_cfg.set(sec, "path", self.url + "/dataset/" + sec);
            self.remote_cfg.set(sec, "type", "remote")
            self.remote_cfg.set(sec, "server", self.url)


class DatasetQuery:
    content_type = "application/octet-stream"

    def __init__(self, dsname, cfg, request, handler):
        self.dsname = dsname
        self.cfg = cfg
        self.request = request
        self.handler = handler
        self.query = request.form.get("query", "").strip()
        self.with_data = request.form.get("with_data", "false").strip() == "true"
        self.sort = request.form.get("sort", "").strip()
        # TODO: arki::utils::net::http::ParamSingle* annotate;
        # TODO: arki::utils::net::http::ParamSingle* command;
        # TODO: arki::utils::net::http::FileParamMulti* postprocfile;
        self.ds = arki.DatasetReader(cfg)
        self.headers_sent = False

    def send_headers(self):
        self.handler.send_response(200)
        self.handler.send_header("Content-Type", self.content_type)
        self.handler.end_headers()
        self.handler.flush_headers()
        self.headers_sent = True

    def run(self):
        origdir = os.getcwd()
        try:
            with tempfile.TemporaryDirectory(prefix="arki-server-") as tmpdir:
                os.chdir(tmpdir)
                self.stream()
                if not self.headers_sent:
                    self.send_headers()
        except Exception as e:
            if not self.headers_sent:
                self.handler.send_response(500)
                self.handler.send_header("Content-Type", "text/plain")
                self.handler.send_header("Arkimet-Exception", str(e))
                self.handler.end_headers()
                self.handler.flush_headers()
                self.handler.wfile.write(str(e).encode("utf-8"))
                self.handler.wfile.write(b"\n")
            else:
                logging.exception("Exception caught after headers have been sent")
        finally:
            os.chdir(origdir)


class DatasetQueryData(DatasetQuery):
    def stream(self):
        self.ds.query_bytes(
            file=self.handler.wfile,
            matcher=self.query,
            data_start_hook=self.send_headers)


class DatasetQueryRepMetadata(DatasetQuery):
    content_type = "text/plain"

    def stream(self):
        self.ds.query_bytes(
            file=self.handler.wfile,
            matcher=self.query,
            with_data=self.with_data,
            sort=self.sort,
            data_start_hook=self.send_headers,
            metadata_report=self.request.form.get("command", ""))


class DatasetQueryRepSummary(DatasetQuery):
    content_type = "text/plain"

    def stream(self):
        self.ds.query_bytes(
            file=self.handler.wfile,
            matcher=self.query,
            with_data=self.with_data,
            sort=self.sort,
            data_start_hook=self.send_headers,
            summary_report=self.request.form.get("command", ""))


class DatasetQueryMetadata(DatasetQuery):
    def on_metadata(self, md):
        if not self.headers_sent:
            self.send_headers()
        md.write(self.handler.wfile)

    def stream(self):
        self.ds.query_data(
            on_metadata=self.on_metadata,
            matcher=self.query,
            with_data=self.with_data,
            sort=self.sort)


class DatasetQueryMetadataInline(DatasetQuery):
    def on_metadata(self, md):
        if not self.headers_sent:
            self.send_headers()
        md.make_inline()
        md.write(self.handler.wfile)

    def stream(self):
        self.ds.query_data(
            on_metadata=self.on_metadata,
            matcher=self.query,
            with_data=True,
            sort=self.sort)


class DatasetQueryPostprocess(DatasetQuery):
    def stream(self):
        # Iterate submitted files and export information about them to the
        # environment
        names = []
        for name, file in self.request.files.items():
            basename = os.path.basename(file.filename)
            file.save(basename)
            names.append(basename)

        if names:
            os.environ["ARKI_POSTPROC_FILES"] = ":".join(names)

        self.ds.query_bytes(
            file=self.handler.wfile,
            matcher=self.query,
            with_data=self.with_data,
            sort=self.sort,
            data_start_hook=self.send_headers,
            postprocess=self.request.form.get("command", ""))


class Handler(BaseHTTPRequestHandler):
    re_pathsplit = re.compile(r"/+")

    def make_environ(self):
        """
        Create an environment that can be used with werkzeug
        """
        # Derived from werkzeug's WSGIRequestHandler
        from werkzeug.urls import url_parse, url_unquote
        from werkzeug._compat import wsgi_encoding_dance

        request_url = url_parse(self.path)

        url_scheme = "http"
        path_info = url_unquote(request_url.path)

        environ = {
            'wsgi.version':         (1, 0),
            'wsgi.url_scheme':      url_scheme,
            'wsgi.input':           self.rfile,
            'wsgi.errors':          sys.stderr,
            'wsgi.multithread':     False,
            'wsgi.multiprocess':    True,
            'wsgi.run_once':        False,
            'SERVER_SOFTWARE':      self.server_version,
            'REQUEST_METHOD':       self.command,
            'SCRIPT_NAME':          '',
            'PATH_INFO':            wsgi_encoding_dance(path_info),
            'QUERY_STRING':         wsgi_encoding_dance(request_url.query),
            'CONTENT_TYPE':         self.headers.get('Content-Type', ''),
            'CONTENT_LENGTH':       self.headers.get('Content-Length', ''),
            'REMOTE_ADDR':          self.client_address[0],
            'REMOTE_PORT':          self.client_address[1],
            'SERVER_NAME':          self.server.server_address[0],
            'SERVER_PORT':          str(self.server.server_address[1]),
            'SERVER_PROTOCOL':      self.request_version
        }

        for key, value in self.headers.items():
            key = 'HTTP_' + key.upper().replace('-', '_')
            if key not in ('HTTP_CONTENT_TYPE', 'HTTP_CONTENT_LENGTH'):
                environ[key] = value

        if request_url.netloc:
            environ['HTTP_HOST'] = request_url.netloc

        return environ

    def dispatch(self):
        """
        Dispatch a request to the endpoint given by self.server.url_map
        """
        from werkzeug.wrappers import Request, Response
        request = Request(self.make_environ())
        adapter = self.server.url_map.bind_to_environ(request.environ)
        try:
            endpoint, values = adapter.match()
            return getattr(self, endpoint)(request, **values)
        except HTTPException as e:
            self.send_response(e.code, e.description)
            for k, v in e.get_headers():
                self.send_header(k, v)
            self.end_headers()
            self.wfile.write(e.get_body(request.environ).encode("utf-8"))

    def do_GET(self):
        self.dispatch()

    def do_POST(self):
        self.dispatch()

    def send_response(self, code, msg=None):
        """
        Wrap BaseHTTPRequestHandler's send_response to add logging of the
        request
        """
        logging.info('%s - - [%s] "%s" %d -',
                      self.client_address[0],
                      datetime.datetime.utcnow().strftime("%d/%b/%Y:%H:%M:%S %z"),
                      self.requestline,
                      code)
        super().send_response(code, msg)

    def arki_index(self, request):
        raise NotFound("TODO: index")
        # local_handlers.add("", new IndexHandler);

    def arki_config(self, request):
        # ./run-local arki-query "" http://localhost:8080
        # curl http://localhost:8080/config
        self.send_response(200)
        self.send_header("Content-Type", "text/plain")
        self.end_headers()
        out = io.StringIO()
        self.server.remote_cfg.write(out)
        self.wfile.write(out.getvalue().encode("utf-8"))

    def arki_dataset_config(self, request, name):
        # ./run-local arki-query "" http://localhost:8080/dataset/<name>
        # curl http://localhost:8080/dataset/<name>/config
        if not self.server.remote_cfg.has_section(name):
            raise NotFound("Dataset {} not found".format(name))
        cfg = configparser.ConfigParser()
        cfg.add_section(name)
        for k, v in self.server.remote_cfg.items(name):
            cfg.set(name, k, v)
        self.send_response(200)
        self.send_header("Content-Type", "text/plain")
        self.end_headers()
        out = io.StringIO()
        cfg.write(out)
        self.wfile.write(out.getvalue().encode("utf-8"))

    def arki_qexpand(self, request):
        # ./run-local arki-query "" http://localhost:8080
        query = request.form["query"].strip()
        expanded = arki.expand_query(query)
        self.send_response(200)
        self.send_header("Content-Type", "text/plain")
        self.end_headers()
        self.wfile.write(expanded.encode("utf-8"))

    def arki_dataset_query(self, request, name):
        if not self.server.cfg.has_section(name):
            raise NotFound("Dataset {} not found".format(name))
        cfg = dict(self.server.cfg.items(name))
        style = request.form.get("style", "metadata").strip()

        # TODO: pmaker.server_side = true;
        # TODO: if (!sort->empty())
        # TODO:     pmaker.sort = *sort;
        #
        # TODO: // Validate request
        # TODO: string errors = pmaker.verify_option_consistency();
        # TODO: if (!errors.empty())
        # TODO:     throw net::http::error400(errors);

        if style == "metadata":
            Streamer = DatasetQueryMetadata
        elif style == "inline":
            Streamer = DatasetQueryMetadataInline
        elif style == "data":
            Streamer = DatasetQueryData
        # TODO: } else if (*style == "yaml") {
        # TODO:     pmaker.yaml = true;
        # TODO: } else if (*style == "json") {
        # TODO:     pmaker.json = true;
        elif style == "postprocess":
            Streamer = DatasetQueryPostprocess
        elif style == "rep_metadata":
            Streamer = DatasetQueryRepMetadata
        elif style == "rep_summary":
            Streamer = DatasetQueryRepSummary
        else:
            raise NotFound("TODO: query style {}".format(style))

        streamer = Streamer(name, cfg, request, self)
        streamer.run()

#    // Response header generator
#    StreamHeaders headers(req, dsname);
#
#    // Set content type and file name accordingly
#    if (pmaker.yaml)
#    {
#        headers.content_type = "text/x-yaml";
#        headers.ext = "yaml";
#    }
#    else if (pmaker.json)
#    {
#        headers.content_type = "application/json";
#        headers.ext = "json";
#    } else if (!pmaker.report.empty()) {
#        headers.content_type = "text/plain";
#        headers.ext = "txt";
#    }
#
#    // If we are postprocessing, we cannot monitor the postprocessor
#    // output to hook sending headers: the postprocessor is
#    // connected directly to the output socket.
#    //
#    // Therefore we need to send the headers in advance.
#
#    {
#        // Create Output directed to req.sock
#        sys::NamedFileDescriptor sockoutput(req.sock, "socket");
#
#        // Hook sending headers to when the subprocess start sending
#        pmaker.data_start_hook = [&](NamedFileDescriptor&) { headers.send_headers(); };
#
#        // Create the dataset processor for this query
#        unique_ptr<runtime::DatasetProcessor> p = pmaker.make(matcher, sockoutput);
#
#        // Process the dataset producing the output
#        p->process(ds, dsname);
#        p->end();
#    }


#    def arki_aliases(self, path):
#        self.send_404("TODO: aliases")
#        # local_handlers.add("aliases", new AliasesHandler);
#
#    def arki_dataset(self, path):
#        self.send_404("TODO: dataset")
#        # local_handlers.add("dataset", new DatasetHandler);
#
#    def arki_query(self, path):
#        self.send_404("TODO: query")
#        # local_handlers.add("query", new RootQueryHandler);
#
#    def arki_summary(self, path):
#        self.send_404("TODO: summary")
#        # local_handlers.add("summary", new RootSummaryHandler);
#
#    #    if (opts.inbound->isSet())
#    #        local_handlers.add("inbound", new InboundHandler(opts.inbound->stringValue()));


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="Start the arkimet server, serving the datasets found in the configuration file")

    parser.add_argument("configfile", help="dataset configuration file")

    parser.add_argument("--host", "--hostname", metavar="host", default="", help="interface to listen to. Default: all interfaces")
    parser.add_argument("--port", "-p", metavar="port", type=int, default=8080, help="port to listen not. Default: 8080")
    parser.add_argument("--url", metavar="url", help="url to use to reach the server")

    parser.add_argument("--accesslog", metavar="file", help="file where to log normal access information")
    parser.add_argument("--errorlog", metavar="file", help="file where to log errors")
    parser.add_argument("--syslog", action="store_true", help="log to system log")
    parser.add_argument("--quiet", action="store_true", help="do not log to standard output")
    parser.add_argument("--verbose", action="store_true", help="verbose output")
    parser.add_argument("--debug", action="store_true", help="debug output")

#        runtest = add<StringOption>("runtest", 0, "runtest", "cmd",
#                "start the server, run the given test command"
#                " and return its exit status");
#        inbound = add<StringOption>("inbound", 0, "inbound", "dir",
#                "directory where files to import are found."
#                " If specified, it enables a way of triggering uploads"
#                " remotely: files found in the inbound directory can be"
#                " remotely scanned or imported");

    args = parser.parse_args()

    root_logger = logging.getLogger()
    if args.debug:
        root_logger.setLevel(logging.DEBUG)
    elif args.verbose:
        root_logger.setLevel(logging.INFO)
    elif args.quiet:
        root_logger.setLevel(logging.ERROR)
    else:
        root_logger.setLevel(logging.WARN)

    if not args.quiet:
        h = logging.StreamHandler()
        h.setFormatter(logging.Formatter("%(asctime)-15s %(levelname)s %(message)s"))
        root_logger.addHandler(h)

    if args.accesslog:
        h = logging.WatchedFileHandler(args.accesslog)
        h.setFormatter(logging.Formatter("%(asctime)-15s %(levelname)s %(message)s"))
        h.setLevel(logging.INFO)
        class Filter:
            def filter(self, record):
                return record.level < logging.WARN
        h.addFilter(Filter())
        root_logger.addHandler(h)

    if args.syslog:
        h = logging.SyslogHandler()
        h.setFormatter(logging.Formatter("%(asctime)-15s %(levelname)s %(message)s"))
        root_logger.addHandler(h)



    #// Initialise setproctitle hacks
    #wibble::sys::process::initproctitle(argc, (char**)argv);

    #commandline::Options opts;
    #try {
    #    nag::init(opts.verbose->isSet(), opts.debug->isSet());

    #    // Configure the server and start listening
    #    ServerProcess srv(opts);
    #    srv.restart_argv = restart_argv;
    #    srv.restart_environ = restart_environ;
    #    srv.log << wibble::log::INFO << "Listening on " << srv.http.host << ":" << srv.http.port << " for " << srv.http.server_name << endl;

    #    if (opts.runtest->isSet())
    #    {
    #        // Fork and start the server
    #        srv.fork();

    #        // No need to poll the server until ready, as the
    #        // socket was already listening since before forking
    #        int res = system(opts.runtest->stringValue().c_str());

    #        srv.kill(SIGINT);
    #        srv.wait();
    #        return res;
    #    } else {
    #        return srv.main();
    #    }
    #} catch (commandline::BadOption& e) {
    #    cerr << e.what() << endl;
    #    opts.outputHelp(cerr);
    #    return 1;
    #} catch (std::exception& e) {
    #    cerr << e.what() << endl;
    #    return 1;
    #}

    httpd = Server((args.host, args.port), Handler)
    httpd.read_config(args.configfile)
    httpd.serve_forever()


if __name__ == "__main__":
    main()
