FATTI
=====

 - Merge configurazioni di un set arbitrario di dataset
 - Estrazione metadati da GRIB1, GRIB2 e BUFR
 - Motore di matching di metadati
 - Dispatch dati in dataset date espressioni di metadati
 - Merge di metadati e summary in summary
 - Grep su uno stream di metadati
 - Collect dei dati puntati da uno stream di metadati
 - Query OR su parametro omogeneo (origin: foo or bar or baz) [sono ancora
   mappabili in SQL]
 - Mappatura sottoquery in SQL
 - Generazione / aggiornamento sommari in tutti i livelli di directory di un
   dataset, senza toccare il timestamp dei summary che non cambiano
 - Query al dataset data un'espressione di match, senza usare gli indici
 - Alias per query livelli, scadenza, prodotti...
   (XGAR_ALIASES or /etc/xgribarch/match-alias.conf)
 - Query al dataset usando gli indici.
 - Metadata compressi
    - header MDvvllll
      vv = version
      llll = length
    - elementi
      ttllll[data]
      tt = type
      llll = length
      [data] = type-dependent
 - Estrazione configurabile di metadati da grib 1 e 2 usando Lua
 - Match di Area e Ensemble come coppie chiave=valore
    + Encoding:
      1 byte lunghezza chiave
      chiave
      1 byte tipo valore (int1, int2, int3, int4 [, int6, int8, float, double,
        string8, string16, string24, string32...])
      valore
    + Match
      chiave=valore[,chiave=valore...]
      eventualmente anche >=, <=, >, <, !=, in
    + Scan
      arki.area.qualsiasicosa = ...
      arki.ensemble.qualsiasicosa = ...
      http://pgl.yoyo.org/luai/i/lua_next
    + Nel database
      encodato in un blob in una tabella separata, match su tutte le righe della
      tabella, poi query IN(a, b, c...) sugli ID di collegamento
      Indicizzazione in sql su tabella a parte (id, blob): si estraggono tutti gli
      elementi della tabella, si fa il match sul blob, si segnano gli id che fanno
      match, si fa una ricerca "IN" sull'indice principale
 - Rendere configurabile cosa è unico:
     unique = origin,reftime
   con un default)
 - arki-server file.conf
 - arki-dump file.log | grep-dctrl -FDataset error
 - dataset di tipo 'outbound'
 - dataset di tipo 'discard'
 - arki-check
 - catene di postprocessazione (--postproc="cmdline")
 - download configurazione da dataset remoti
 - report scriptabili (arki-query --report)
 - cancellazione dati dai dataset (arki-check --remove)
 - simulazione di dispatch (arki-scan --testdispatch)


Usi utili:
 - Crea la configurazione per l'accesso a un certo numero di dataset:
   collectdsconf ../testenv/* > conf
 - Importa dei grib:
   arki-scan-grib *.GRIB | arki-dispatch conf > dispatched
   arki-update-summaries conf
 - Filtro per file GRIB:
   arki-scan-grib *.grib | arki-grep expr | arki-collect > filtered.grib
 - Sommario del contenuto di vari GRIB
   arki-scan-grib *.grib | arki-summarise --now=0
 - Query a uno o piú dataset
   arki-query conf expr



DA FARE
=======

Annunciati:
 + Bounding box coordinate ruotate
 + I grib latlon ruotati (con latitudeOfSouthernPoleInDegrees e
   longitudeOfSouthernPoleInDegrees) vanno antiruotati
 + In arki-mergeconf, usare lua per calcolare bounding box (in maniera
   accurata) a partire dalle definizioni delle aree
 + Accorciata la query al database tenendosi in memoria tutti i metadati
 + Run stubborn queries in the index and pretty much everywhere: "database
   is locked" should not be a failure model
Da annunciare:
 + Ondisk2
 + SQLITE_BUSY is returned also on compiles
   Instead, set a timeout handler:
   http://www.sqlite.org/c3ref/busy_handler.html
   http://www.sqlite.org/c3ref/busy_timeout.html
 * ondisk2 project
    + datafile and index: proof of concept ready
    + integrate datafile and index into writer
    - start testing imports
       - to make them work
       - to see the performance
       - to see the index size
    + data queries
    + summary generation
    + during read, build the file list first then close the read query: that
      way:
       + slow readers can release database locks quickly
    + lasciare aperti i datafile e chiuderli al flush
    + fdatasync
    + add notes and other non-summarisable metadata to the index
    + repack:
       + for each file
          + lock away reads and writes
          + make a copy of the file with the right data in it
          + rename the file with the final name
          + delete all records about the file from the db
          + insert the new records into the db
          + commit
          + unlock writes
    + controllo file che sono nell'indice ma cancellati da disco
    + rebuild:
       + for each file
          + if not in the db, rescan it
          + if in the db
             + if there are records pointing outside the file, rescan it
	     - OPTIONAL if there are records pointing to data in the file that
	       cannot be parsed, rescan it
       + for each record in the db without a file
          + remove record in the db
    = is it possible to do an index.sqlite per directory, then aggregate them
      if/as needed with the add database feature? No: the aggregation would nor
      merge tables, but require them to be accessed as db.something
    = save a copy of the total summary at flush time. Use it if no reftime: is
      present in the summary query instead of performing the aggregated query
      in the database
       - after add operations, just add to the existing summary
       - after delete/replace operations, regenerate the summary from the index
         at flush time
       - at check time, generate the summary from the index if it is missing,
         or if the index has been changed
      (too much work to keep it up to date, it's easier to generate it at read
      time and check its timestamp against the index)
    + delete the summary on write and create it on read (no need to delete on
      write, because on read the timestamp is checked against the index)
    + honour the index config variable: we do not need an index for everything
      in order to do summaries, and it's good to reduce the chances of sqlite
      picking a stupid index
    + when querying the summary, if no 'reftime' is present, use the global
      one. If 'reftime' is present use the index
    + when doing repack, also open with journal_mode=truncate (if vacuum does
      not do it already) and report the savings after the journal trim
    + perform ANALYZE on repack/check
    + rebuild the cached summary after repack/check
    + il detect della roba di cui far pack continua ad avere falsi positivi

---

 - vedere se è possibile calcolare i minimi/massimi dei vari lati della griglia
   tramite bisezione sul lati (per beccare tutte le proiezioni che hanno un
   solo massimo/minimo per lato)
 - Guardare con Selvini i LUA che estraggono i BBOX
 = Togliere il bounding box dai metadati
 = Bbox BOX: prendere {lat,lon}{first,last} invece di {min,max} perché suona
   meglio e non pone il problema di decidere qual'è il minimo e massimo
 = Bbox BOX: stampare i numeri con N, S, W, E invece che solo una lista
 = Definire come creare un matcher sul bbox in arkimet


http://www.unidata.ucar.edu/committees/polcom/2005fall/THREDDS20050912.htm
http://www.ecmwf.int/products/data/software/simdat.html
http://code.ecmwf.int/trac/vmc
http://www.openchannelsoftware.com/projects/Earth_Science_Datacasting
http://code.ecmwf.int/trac/vmc/wiki
http://code.ecmwf.int/trac/vmc/wiki/MetadataGuidelines
http://code.ecmwf.int/trac/vmc/wiki/TestingScenarios#TestingScenarios
 - Vedere se/come interfacciarsi a LDM
   http://www.unidata.ucar.edu/software/ldm/
   http://www.unidata.ucar.edu/software/ldm/ldm-6.6.5/

Query sottoaree (ISAC)
 + matcher per BBox
 + il type::Area con un mutable per il bbox per non doverlo ricalcolare
 + mettere direttamente il WKT nella sintassi del matcher sul bbox
 + il matcher bbox deve diventare un matcher sull'area
 + types::BBox deve sparire
   (se non rompe il formato binario, sparire del tutto, anche dai summary)
 + i BBox non sono salvati nel database, ma sono calcolati al volo dalle aree.
   Quindi, al momento il match sui bbox funziona solo per modo di dire, perché
   non ha niente da matchare nei metadati, nei summary e nei dataset, quindi
   restituisce sempre un risultato vuoto
    - nel database, se si matcha un bbox si tirano fuori le aree, si calcolano
      i loro bbox e si matcha sulle aree
    - nei summary, implementare una funzione che aggiunge bbox ai summary
    - nei metadata, se si fa get di bbox, calcolarlo al volo
 + arki-retrieve to gridspace
 + add --gridspace=file (where file is passed to Gridspace::read) to runtime.cc
   to be used in arki-scan and arki-query
 + gridspace: fare query del summary dei dati dati senza i matcher, per evitare
   di avere errori di matcher che non matchano quando in realtà sono stati
   richiesti metadati assurdi
 + match one (espande con un solo elemento)
 + match all existing (espande con tutte le possibilità)
 + query metadata instead of summary
    + query metadata matching the pure items given, plus the "match only"
    + then run any/all expansions and add the items
    + if the metadata are local, no need to requery: filter that
    - if the metadata are remote, requery trying to condense reftime matchers
      (for the "match all of", use the original matcher expression, and maybe
      skip the redundant pure items that match it)

 - per il reftime, espandere senza guardare il dataset, per beccare buchi
 - match only (semplicemente aggiunge la query al resto, ma non fa filtraggio
   successivo)
 - arki-dump --gridspace=file dataset
   show all gridspace steps
 - match all reftime: ...
   espande in TUTTI i risultati del match
   (a sto punto invece di querare i summary querare direttamente i metadati,
    tenerli in memoria e farci sopra i conti)
    - avendo tutti i metadati in memoria, si può anche far caso a che tipo di
      source hanno: se hanno source locali, non importa rifare la query per
      avere i dati
 - dispatch in file il cui nome dipende dal dato
    - --multioutput="tipo:pattern"
    - tipo è un file lua in /etc/arkimet/multioutput/
    - pattern è passato alla variabile global "pattern"
    - il file parsa il pattern e restituisce una funzione (md)->string
    - dataset sincronizzato con un Output, quando dà fuori un dato, cambia
      l'output al file corrispondente
    - lua per generare i nomi di file
    - un qualche sistema per testare la cosa
 - annunciare
 - upload in Debian
    - build con wibble di sistema
    - documentazione, manpage, descrizioni in debian/control...

Autenticazione (Bider)
 - autorizzare o no a livello di archivio
 - anche magari su host locale
 - associazione utente->archivi che può vedere
 + --restrict=tag --restrict=tag1,tag2
   (restrict to datasets visible by the given user)
 + usare in arki-query, arki-scan, arki-check, arki-mergeconf
 - arki-server: passare --restrict= dall'env di configurazione
 - arki-query con opzioni per l'autenticazione https

Formatter (Emanuele)
 - verificare nei test che errori di sintassi o logici dei formatter non
   piantino tutto
 - arki-dump che testa il formatting di un metadato
 - dar tutto in mano a Emanuele edigiacomo@arpa.emr.it
   (come testare: con env var che punta a formatter locali)

 - a volta ci sono falsi positivi di TO_PACK

 - lm7tmpc eccezione maskata da sqlite

Minguzzi:
Se posso esprimere una preferenza, le mie priorita' sarebbero a2 (alias) e D (bufr qualita' aria)
 - spostare --merged lato server (e farlo solo tra dataset che stanno sullo
   stesso server)
    - Mergia il possibile lato server, poi rimergia lato client se piglia da
      server diversi
       - Se si rimergia lato client, rifare il sort
 - TimeIntervalSorter: permettere di configurarlo in modo che se ci sono 2 dati
   con lo stesso reftime e altri metadati, permetta di scegliere se darli in
   output tutti o se dare in output solo quello del primo dataset che ce l'ha
   (solo nel caso di multidataset, va rivisto a cosa serve, perché se serve
   come precondizione al postprocessing, non si facomunque postprocessing nel
   multidataset)
   Caso d'uso: quero i modelli multidataset, l'operativo e il backup. Se non
   c'è il dato dell'operativo, lo prendo dal backup, ma se c'è il dato in
   entrambi, voglio solo il primo.
 + postprocessing e reporting sono per-dataset
 + Alias: come faccio a sapere cosa c'è sul server?
   arki-dump --aliases

Bufr:
 - bufr: aprirli se esiste la tabella, altrimenti non matcheranno e finiranno in error
 - solo header (altrimenti bisogna separare BUFR con piú subset perché
   potrebbero avere orari differenti (o altre informazioni discriminanti)
    - si possono anche aprire: se entrano BUFR che vanno smistati a livello di
      subset invece di dataset, li si preprocessa prima di importarli
 - altre cose possibili sono:
    - la sequenza di descriptor della data section (senza espansione dei D)
    - la versione delle tabelle (?)
    - il numero di subset (?)
 - fare per i bufr un dataset non indicizzato e senza metadati, solo coi summary
    - fare un programma che fa pulizia di un file di dati cancellando i bufr
      duplicati
    - a ogni scrittura, creare il flagfile .needs-repack, cosí la pulizia
      viene fatta girare solo sui file "dirty"
    - ragionare se fare un indice solo con reftime e md5
 - estremi di lat,lon (opzionale)
 - estrazione dei BUFR dentro a un DB-All.e
 - ignorare il problema dei bufr multisubset, perché si possono sempre
   esplodere prima dell'import
 - usare i classici metadati DB-All.e come metadati: lat, lon, year, month,
   day, hour, minu, sec, rep_memo
    - lavorare a livello di dba_msg a questo punto
    - i metadati però diventano piú grossi dei dati

(12:08:20) spanezz@jabber.linux.it/Home: ma quello che mi dici ora significa che praticamente io se voglio esportare un anno di una stazione, devo esportare un anno di TUTTE le stazioni, e poi a valle filtrare
(12:08:30) spanezz@jabber.linux.it/Home: a valle == sul client
(12:08:55) pat1@jabber.linux.it: io pensavo a valle con un postprocessatore sul server 
(12:08:57) spanezz@jabber.linux.it/Home: perché se si decide di non fare esplodere arkimet coi bufr strani, io non li parso, mai
(12:09:24) pat1@jabber.linux.it: io pensavo alla stessa identica logica dei grib
(12:09:24) spanezz@jabber.linux.it/Home: ah, ok, se volete si può fare anche cosí
(12:09:43) spanezz@jabber.linux.it/Home: a quel punto posso importare anche i bufr multisubset, e il vostro postprocessatore sceglie i subset che vuole
(12:09:58) pat1@jabber.linux.it: bhe pensiamoci
(12:10:11) pat1@jabber.linux.it: io davo per scontata questa strada
(12:10:18) pat1@jabber.linux.it: ma se ce ne sono altre bene
(12:10:18) spanezz@jabber.linux.it/Home: nel qual caso, già va

Dati fuori linea:
 - fare un source "on media <nome>"
 - nell'archive, se viene querato un archivio fuori linea, generare metadati
   "virtuali" dal dataset, col source preso da "on media <nome cartella>"
 - tenere online .metadata e .summary
 - le query metadati e summary e report, rimangono ok
 - le query solo dati, danno un valore di ritorno != 0 ma con un valore
   specifico che dice "ci sarebbero altri dati, ma sono archiviati" (e magari
   un messaggio in stderr)

Messe a punto:
 - pare che INDEXED BY indice funzioni solo con SQLite nuovi
 - bbox dipende da runtime: runtime::rcFiles -> utils/files
 - Alias
    - possibilità di fare include del file degli alias
    - permettere di includere gli alias, con un URL che punti a un server
    - cachare l'alias remoto in una directory e tenerlo buono per un certo 
      numero di ore (o se il server non risponde)
 - Quando arki-check fa il rebuild perde le note, e quindi l'informazione sul
   file originale da cui è stato fatto l'import.  Lí non si può ovviare perché
   non possiamo fidarci dei metadati; è però il caso di segnare "original file
   name lost during dataset rebuild"
 - Ricostruzione indici ondisk1 dopo la cancellazione di file
   - select distinct file from md
   - guarda uno per uno se esistono
   - se ne manca uno, togliere i suoi dati dall'indice
 - non creare needs-check-do-not-pack se index.sqlite non esiste ma non
   esistono neanche file scannabili
 - usare scan::scan in tutte le letture sequenziali (arki-scan, rebuild di
   dataset, scansioni sequenziali di dataset senza indice, ...)
 - spostare tutte le funzioni codec::decode* dentro a codec::Decoder
 - Archivio:
    - nel rebuild, se un file contiene dati interamente precedenti all'archive
      age, creare il suo .summary e aggiungerlo a mdarc invece che all'indice
      principale
    - nell'acquire / replace / delete, confrontare con archive_age e delete_age
      non si fa delete o replace o append di dati in mdarc: dare errore se si
      prova a farlo. La roba precedente all'archive age diventa di fatto read only
 - fare un comando per spostare, rinominare e concatenare i file grib
   all'interno dei dataset
 - calcolare sempre gli estremi e mettere all'inizio della query reftime un
   >=X, <=X o BETWEEN(x,y), che danno a sqlite la possibilità di usare gli
   >indici
 - arki-query run/conf dà segfault (senza usare -C prova a leggerlo come un
   dataset su file)
   (questo è perché per ogni file sconosciuto assume il formato "arkimet")
 - arki-server: mettere ulimit a arki-query quando lo si lancia
 - arki-server: genera un feed RSS per dataset con gli ultimi arrivi
 - i file letti come dataset potrebbero rendere obsoleti arki-grep, arki-scan-*
   e arki-dump
 - arki-check: fare un progress report se si esegue interattivo: per file
   grossi, sta lí a non far niente per un bel po' e ci starebbe almeno sapere
   che file sta checkando
 - Invalidate dovrebbe funzionare anche se metadata e summary e indice
   contengono rusco
 - Fare un invalidate solo per summary e indice
   Fare un invalidate che salva file .metadata per i dati che ancora vanno
   bene, da usare per ricostruzioni dell'indice in caso di cambio del formato,
   cosí si preservano le note.  (ma in caso di cambio del formato, cosa me ne
   faccio dei .metadata?)
 - Query con reftime esatto sull'estremo di un summary (e un'origin fissata
   presa dal summary) non dà risultati 
   (non riesco a riprodurlo: non è che in quel summary c'è piú di un'origin, e
   si fissa un'origin che non esiste negli estremi?)
 - I percorsi assoluti dei dataset, quando si estraggono dati, acquisiscono un
   punto davanti, diventando relativi
   (non riesco a riprodurlo)
 - Semplificare le query: se vedi "reftime:" senza sottoespressioni, togliere
   quel ramo
 - In arki-query, aggiungere --allok per dire 'si, va bene la query vuota'; e
   per default, se la query (semplificata) non seleziona niente, rifiutarsi di
   eseguire
 - Manpage 'arkimet' con:
    - un'introduzione generale
    - descrizione e puntatori alle manpage dei vari comandi
    - puntatore alla manpage con la sintassi del matcher
    - puntatore alla manpage con la sintassi dei file di configurazione e degli
      alias
 - Aggiungere agli errori un link al wiki
 - Grafici di come sono processati i dati a seconda di vari comandi di
   arki-query e arki-scan

Importazione con tag:
 - quando si importa, passare una stringa arbitraria che identifica in maniera
   extra tutti i dati importati in quel file
 - la stringa, farla diventare un path, cosí anche se non è scannabile dal
   file, è discernibile dai path
 - permettere di querare per path o path parziale
   (select from md where file like "STRINGA/%")

Ondisk1:
 - controllare l'uso del match sui null in ondisk1 (select =? becomes =NULL
   that is always false)
 - controllare/deindicizzare i file cancellati al repack di ondisk1
   (solo se c'è replace: yes)

 - permettere di far acquire (ma non delete o replace) in archive, e usarlo per
   duplicates, errors e outbound invece di ondisk1
   (o fare merge di ondisk1 e archive da usare solo per duplicate, errors e
   outbound)

 - ondisk2: sort prima della roba che usa l'indice e poi di quella che non lo
   usa

 - paola: è possibile modificare l mdstat o crearne uno simile che mi includa
   anche le descrizioni?

 - cancellazione di dati data un'estrazione di metadati

 - script che fa setup di un sistema di partenza

 - dataset "offline", che tenga solo i file .summary (toplevel, per directory,
   per file)

Note:
 - replica di dataset con però dati bucati
   find dataset -type f -not -name "lock" -not -name "config*" -not -name "*summary" -not -name "*.metadata" -not -name "*.sqlite" -not -name "*.grib1" -not -path "*/.svn*"
   find . -type f -not -name "lock" -not -name "*.grib1" -not -path "*/.svn*" -not -name "*~" -printf "%s\n" | numsum
   find . -name "*.grib[12]" -printf "%s %p\n"
   tar --files-from metadata.filelist -zcf metadata.tar.gz
 - Conversione ondisk->ondisk2
   # Cambia type= rompendo gli hard link
   find . -maxdepth 2 -name config | while read name; do sed -e 's/type = local/type = ondisk2/' $name > $name.ondisk2; done
   find . -maxdepth 2 -name config | while read name; do if [ -e $name.ondisk2 ]; then mv $name.ondisk2 $name; fi; done
   # Toglie 'index'
   sed -i '/^index/d' */config
   # Cancella i summary
   find . -name "*summary" -delete
   # Cancella gli indici
   rm */index.sqlite*
   # Lascia i metadati, cosí arki-check li può riusare

 - Passi futuri:
    - far andare arkimeow su nemo coi plugin in modo utile
    - procedure di Minguzzi
    - integrazione procedure di Minguzzi in arkimeow

Triggering:
 - Poll:
   arki-query --summary --report=count "reftime:=today; run:MINUTE,0; timerange:GRIB1,0,120h or GRIB1,0,150h" http://arkimet.metarpa:8090/dataset/eur025
 - Dataset toccati dall'ultimo import (ev. fare un reportino lua per estrarre la cosa):
   arki-dump 2008-10-15_10-metadata.log --report=datasets | cut -f 2 -d ' '
   arki-dump 2008-10-15_10-metadata.log --report=datasets | cut -f 2 -d ' ' | xargs -ipippo touch /tmp/importati/pippo
 - arki-query --watch
   che ripete la query all'infinito ogni volta che arriva qualcosa nel dataset
   pigliando notifiche da arki-scan e chiunque importi
 - trigger per-dataset da lanciare dopo la query
    - una query + un programmino in lua che decide se il trigger va lanciato +
      vedere cosa vuol dire lanciare un trigger (e, nel caso, non rilanciarlo
      due volte)




 - Audio, con arki-query che genera playlist (file:// o http://)


Whenever I do the image indexing experiment, I can use xmlindexer from strigi
to extract XML metadata; a LUA script to dispatch it to Arkimet metadata, and a
strigi plugin to connect the arkimet datasets to strigi desktop search (that
would give a compact, archivable photo archive indexable together with the rest
of desktop search)

If, during extraction, I sort SQL results by source file, then I can extract
from inside tarballs or zipfiles, by just caching only the last expanded
version somewhere during the retrieval

ZIP could use a common dictionary, but it's unknown if any client can do it
the new google archive format possibly can as well

gzip can use an external dictionary for encoding: in theory, I can do
compressed archives using an external common dictionary, and then everything
else is linear streams

squashfs seems to work nicely, but it needs to be tested with lots of BUFR files

Dataset "un file per dato", per dati grandi (e foto)

Usando il Source URL, vedere se guadagnamo gratis l'accesso a NOMADS

arki-server: generazione risultati query usando Source URL

Catalogo di 2o livello
 - input: conf di tutti i dataset da indicizzare
 - ottiene lui i summary
 - output: conf dei dataset che fanno match (pronta per essere passata ad arki-query)
 - estrazione summary da mars

Semplificazione di un matcher dato un summary, rimuovendo dagli OR i valori che
non ci sono nel summary (e saltando la query se una subquery rimane senza
nessun OR).

alias in or: product:t* diventa product:ta or tb or tc...

GRIB2
 - Usare il tmpfile SOLO se si sta leggendo un multigrib (se è possibile
   saperlo)
    - patch a grib_api per sapere se si sta lavorando con un multigrib
    - patch a grib_api per sapere offset e dimensione del grib nel file

Query "Timerange != da 1,0,0" o "Prodotto != 21"

XGRIB
 - Alias di xgrib
    - livelli e scadenze già scaricati (file *memo* in DAT.tar.gz)
    - bider mi deve dare le variabili
 - Conversione query xgrib (lavoro fatto anche da Lazzeri)
    * ricerca per:
       * nome del dataset (obbligatorio)
       * data di emissione (obbligatorio)
          * una
          * una lista di date
          * tante (data inizio, data fine, step)
          * una data può essere definita come data o come:
             * yesterday (+/- nn giorni)
             * today (+/- nn giorni)
       * ora di emissione (obbligatorio)
          * una
          * una lista
          * (ora inizio, ora fine, passo in ore)
       * scadenza (opzionale)
          * una o piú
          * wildcard sui mnemonici
       * livello (opzionale)
          * uno o piú
          * wildcard sui mnemonici
       * variabile e codici estensione local definition (opzionale)
          * una o piú
          * wildcard sui mnemonici

Indicizzare aree
 Fare prove con GDAL di intersezioni proiezioni strane
 (solo per il metaindice: per l'import basta fare match letterale)
 Scan e match e summary aree
 - OGRGeometry (da OGR in GDAL) ha metodi tipo intersect (ma vuole libgeos)
   OGR supporta coordinate ruotate
   import gdal,ogr
   a = ogr.osr.SpatialReference()
   a.SetMercator(0,0,100,0,0)
   b = ogr.osr.SpatialReference()
   b.SetPS(0,0,100,0,0)
   t = ogr.osr.CoordinateTransformation(a,b)
   t.TransformPoint(10,10)
   r = ogr.Geometry(ogr.wkbLinearRing)
   r.AddPoint_2D(10,10)
   r.AddPoint_2D(10,100)
   r.AddPoint_2D(100,100)
   r.AddPoint_2D(100,10)
   r.CloseRings()
   r.AssignSpatialReference(a)
   r.ExportToWkt()
   r.GetSpatialReference().ExportToWkt()
   s = r.Clone()
   s.TransformTo(b)
   r.Touches(s)
   r.Intersect(s)

Pensare a "validity time" come reftime + timerange
